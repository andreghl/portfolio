<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Andre-Ignace Ghonda Lukoki" />

<meta name="date" content="2024-06-01" />

<title>A Linear Regression</title>

<script src="lib/header-attrs-2.26/header-attrs.js"></script>
<link href="lib/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="lib/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lib/highlightjs-9.12.0/highlight.js"></script>
<!DOCTYPE html>
<html lang="en">
    <head>
      <meta charset="UTF-8" />
      <meta
        name="description"
        content="Economics student interested in Econometrics, Statistics, Actuarial sciences, Statistical 
        programming and Economic research."
      />
      <meta name="keywords" content="OLS, Linear Regression, Econometrics, Regression,
      Statistics, Julia, Ordinary Least Squares, Proof, Mathematics, Matrix" />
      <meta name="author" content="Andre-Ignace Ghonda Lukoki" />
      <meta name="viewport" content="width=device-width, initial-scale=1" />

      <script>
        window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };
      </script>
      <script defer src="/_vercel/insights/script.js"></script>
      <script>
        window.si = window.si || function () { (window.siq = window.siq || []).push(arguments); };
      </script>
      <script defer src="/_vercel/speed-insights/script.js"></script>
      <link rel="shortcut icon" href="../assets/logo.svg" />
</head>

<body>
    <div id="menu">
      <span><img src="../assets/back.svg"><a href="../index.html"> Go Back </a></span>
    </div>
</body>
</html>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="../resources/style.css" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">A Linear Regression</h1>
<h4 class="author">Andre-Ignace Ghonda Lukoki</h4>
<h4 class="date">June 1, 2024</h4>



<div class="description">
<p>The linear regression is the most fundamental technique used in Econometrics. Its popularity stems from the interpretability of its coefficients, and the numerous possibilities of extension. Regressions aim to predict the value of a continuous dependent variable based on a series of regressors. In this article, I summarize the knowledge I have acquired on linear regression analysis through university courses and self-study.</p>
</div>
<p><label for="tufte-mn-1" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;">I find <a
href="https://youtu.be/YIfANKRmEu4?si=RVnUKshFZ6eB_iYD">this video</a>
by <strong>Mutual Information</strong> to be a nice introduction to
linear regressions.</span></span></p>
<p>Econometric modeling attempts to estimate the relationship between a set of independent variables or <strong>features</strong>, denoted by <span class="math inline">\(X = (X_{1}, X_{2}, \dots, X_{p })\)</span>, and a <strong>response</strong> variable <span class="math inline">\(y\)</span>. The parameters <span class="math inline">\(\beta_{p + 1}\)</span> limit the set of possible relationships to those that are linear. The econometrician chooses the features to include and the hypothesized functional form of the model for either inference or prediction purposes. The data is assumed to be the realization of an unobservable <em>data-generating process</em> (DGP) from which we know nothing but the probability distribution of its constituents.</p>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<p>The simple regression model is used to study the relationship between a single independent variable and a predictor. The population model is assumed to have the following functional form:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} \qquad (i = 1, \dots, n)
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept of the model, <span class="math inline">\(\beta_{1}\)</span> shows the average change in the dependent variable associated with a one unit change in the value of the regressor, and the error is represented by <span class="math inline">\(\epsilon_{i}\)</span>. The data does not usually stands on a perfect line, the error term <span class="math inline">\(\epsilon_{i}\)</span> represents the deviations from the linear DGP, which are assumed to follow the standard Normal distribution. This error term, however, is more than simple noise when it comes to estimation. Most estimation methods for the linear regression aim to minimize a function of the errors. In this article, we focus on the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares</a> (OLS) method that minimizes the <em>residual sum of squares</em> (RSS) defined as</p>
<p><span class="math display">\[
RSS = \sum_{i = 1}^{N} e_{i}^{2} = \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})^{2}
\]</span></p>
<p><label for="tufte-mn-2" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;">The Ordinary Least Squares method was developed in an era where
computational methods were not powerful enough to minimize the least
absolute deviations. The Least Squares method is also guaranteed to have
at most one solution if the number of observations is greater than the
number of independent variables.</span></span></p>
<p>The OLS estimators for the intercept and the slope are defined as the set of input values at which the minimum of <span class="math inline">\(RSS(\beta_{0}, \beta_{1})\)</span> is found.</p>
<p><span class="math display">\[
\begin{split}
\hat{\beta_{0}}, \hat{\beta_{1}} &amp;= \operatorname{argmin}_{\beta_{0}, \beta_{1}} RSS(\beta_{0}, \beta_{1})
\\
&amp;= \operatorname{argmin}_{\beta_{0}, \beta_{1}} \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})^{2}
\end{split}
\]</span></p>
<p>The solution for the first coefficient is found by setting the first derivative of the residual sum of squares with respect to <span class="math inline">\(\beta_{0}\)</span> equal to zero.</p>
<p><span class="math display">\[
\begin{split}
\frac{d RSS(\beta_{0}, \beta_{1})}{d \beta_{0}} &amp;= - 2 \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})
\\
0 &amp;= \sum_{i = 1}^{N} y_{i} - n \beta_{0} - \beta_{1} \sum_{i = 1}^{N}  x_{i}
\end{split}
\]</span>
The solution is then:
<span class="math display">\[
\beta_{0} = \frac{1}{n} \sum_{i = 1}^{N} y_{i} - \frac{1}{n} \beta_{1} \sum_{i = 1}^{N}  x_{i} = \bar{y} - \beta_{1} \bar{x}
\]</span>
The derivation of the estimator for <span class="math inline">\(\beta_{1}\)</span> follows the same step as the previous derivation, take the derivative and set it equal to <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\frac{d RSS(\beta_{0}, \beta_{1})}{d \beta_{1}} = \sum_{i = 1}^{N} -2 x_{i} (y_{i} - \beta_{0} - \beta_{1} x_{i})
\\
0 =  -\sum_{i = 1}^{N} x_{i} y_{i} + \beta_{0} \sum_{i = 1}^{N}  x_{i} + \beta_{1} \sum_{i = 1}^{N} x_{i}^{2}
\end{split}
\]</span></p>
<p>The solution for the intercept is substituted into the equation,</p>
<p><span class="math display">\[
\begin{split}
\beta_{1} \sum_{i = 1}^{N} x_{i}^{2} = \sum_{i = 1}^{N} x_{i} y_{i} - (\bar{y} - \beta_{1} \bar{x}) \sum_{i = 1}^{N}  x_{i}
\\
\beta_{1} \sum_{i = 1}^{N} x_{i}^{2} - \beta_{1} \bar{x} \sum_{i = 1}^{N}  x_{i} = \sum_{i = 1}^{N} x_{i} y_{i} - \bar{y} \sum_{i = 1}^{N}  x_{i}
\\
\end{split}
\]</span>
resulting in the following solution for the slope coefficient:
<span class="math display">\[
\beta_{1} = \frac{\sum_{i = 1}^{N} x_{i} y_{i} - \bar{y} \sum_{i = 1}^{N}  x_{i}}{\sum_{i = 1}^{N} x_{i}^{2} - \bar{x} \sum_{i = 1}^{N}  x_{i}}
\]</span></p>
</div>
<div id="multivariate-linear-regression" class="section level2">
<h2>Multivariate Linear Regression</h2>
<p>In the multivariate case, the model can better be represented using matrix notation in the following way:</p>
<p><span class="math display">\[
y = X \beta + \epsilon
\]</span></p>
<p>where <span class="math inline">\(y\)</span> in a vector of size <span class="math inline">\(n\)</span>, <span class="math inline">\(X\)</span> is a <span class="math inline">\(n \times m\)</span> matrix, <span class="math inline">\(\beta\)</span> is <span class="math inline">\(m\)</span>-vector, and <span class="math inline">\(\epsilon\)</span> is an other vector of size <span class="math inline">\(n\)</span>. <span class="math inline">\(m\)</span> is simply the number of independent variables <span class="math inline">\(p\)</span> plus one, thus the matrix <span class="math inline">\(X\)</span> contains <span class="math inline">\(p\)</span> regressors and a vector of ones to estimate the intercept. In matrix notation, the coefficients are also estimated by minimizing the sum of squared residuals <span class="math inline">\(e&#39;e = (y - X \beta)&#39;(y - X \beta)\)</span> with respect to <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\phi (\beta) &amp;= (y - X \beta)&#39; (y - X \beta)
\\
&amp;= y&#39;y - y&#39; X \beta - \beta&#39; X&#39; y + \beta&#39; X&#39; X \beta
\\
&amp;= y&#39;y - 2 \beta&#39; X&#39; y + \beta&#39; X&#39; X \beta
\end{split}
\]</span></p>
<p>To find the minimum, the derivative of <span class="math inline">\(\phi(\beta)\)</span> must be set equal to <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\frac{d}{d \beta}(\dots) = -2X&#39;y + 2 X&#39;X \beta &amp;= 0
\\
2X&#39;X \beta &amp;= 2X&#39;y
\\
\beta &amp;= (X&#39;X)^{-1}X&#39;y
\end{split}
\]</span></p>
<p>The <code>LinearAlgebra</code> package<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> The documentation of the package can be accessed <a href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/">here</a>.</span> makes it easy to manipulate vectors and matrices in Julia. Once the package is loaded, and the variables are defined, the next step is to create a function that takes two arguments, the response variable <span class="math inline">\(y\)</span> and the matrix <span class="math inline">\(X\)</span> of regressors with a vector of ones as the first column. The coefficients are calculated according to the least squares formula derived above.</p>
<pre class="julia"><code>function lm(y::Vector, X::Matrix)

    beta = inv(X&#39;X)X&#39;y
    return beta
end</code></pre>
<p>In the previous algorithm, the matrix <span class="math inline">\(X\)</span> is assumed to include a vector of ones as the first column, and the matrix of regressor in the next columns. The Ordinary Least Squares regression does not require the presence of an intercept to estimate the remaining coefficients.</p>
<p><span class="math display">\[
X = \begin{bmatrix}
1 &amp; x_{11} &amp; \dots &amp; x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{bmatrix}
\]</span></p>
<p>It is common practice, however, to include an intercept in the regression. The significance of its coefficient will tell us whether it is useful to keep the intercept in the model. The significance of a coefficient in a linear regression can be tested using a <strong>t-statistic</strong>. This test statistic has the following form:</p>
<p><span class="math display">\[
t_{\hat{\beta}} = \frac{\hat{\beta} - \beta}{\text{se}(\hat{\beta})} \qquad \text{where} \ H_{0}: \beta = 0
\]</span></p>
<p>The null hypothesis presented above is the standard approach in many statistical packages and simply tests for the significance of the coefficient in the model. This test can easily be extended to test for the significance of any (null) hypothesis.</p>
<pre class="julia"><code>Tstat = (beta - 0) ./ betaSE</code></pre>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-Magnus" class="csl-entry">
Magnus, J. R. (2021). <em>Introduction to the theory of econometrics</em> (6th ed.). VU University Press. <a href="https://vuuniversitypress.com/product/introduction-to-the-theory-of-econometrics/?lang=en">https://vuuniversitypress.com/product/introduction-to-the-theory-of-econometrics/?lang=en</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
