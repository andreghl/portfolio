<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Andre-Ignace Ghonda Lukoki" />


<title>A Linear Regression</title>

<script src="lib/header-attrs-2.26/header-attrs.js"></script>
<link href="lib/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="lib/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="lib/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="lib/highlightjs-9.12.0/highlight.js"></script>
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta
      name="description"
      content="Economics student interested in Econometrics, Statistics, Actuarial sciences,  
      Programming and Economic research."
    />
    <meta
      name="keywords"
      content="OLS, Linear Regression, Econometrics, Regression,
      Statistics, Julia, Ordinary Least Squares, Proof, Mathematics, Matrix"
    />
    <meta name="author" content="Andre-Ignace Ghonda Lukoki" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <script>
      window.va =
        window.va ||
        function () {
          (window.vaq = window.vaq || []).push(arguments);
        };
    </script>
    <script defer src="/_vercel/insights/script.js"></script>
    <script>
      window.si =
        window.si ||
        function () {
          (window.siq = window.siq || []).push(arguments);
        };
    </script>
    <script defer src="/_vercel/speed-insights/script.js"></script>
    <link rel="shortcut icon" href="../assets/logo.svg" />
  </head>

  <body>
    <span id="top"
      ><img src="../assets/back.svg" /><a href="../index.html">
        Go Back
      </a></span
    >
  </body>
</html>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="../resources/style.css" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">A Linear Regression</h1>
<h4 class="author">Andre-Ignace Ghonda Lukoki</h4>
<h4 class="date">08-28-2024</h4>



<div class="description">
<p>The linear regression is the most fundamental technique used in Econometrics. Its popularity stems from the “interpretability” of its coefficients, and the numerous possibilities of extension. Regressions aim to predict the value of a continuous dependent variable based on a series of regressors. In this article, I summarize the knowledge I have acquired on linear regression analysis through university courses and self-study.</p>
</div>
<p>Econometric modeling attempts to estimate the relationship between a set of independent variables or <strong>features</strong>, denoted by <span class="math inline">\(X = (X_{1}, X_{2}, \dots, X_{p })\)</span>, and a <strong>response</strong> variable <span class="math inline">\(y\)</span>. The parameters <span class="math inline">\(\beta_{p + 1}\)</span> limit the set of possible relationships to those that are linear. The econometrician chooses the features to include and the hypothesized functional form of the model for either inference or prediction purposes. The data is assumed to be the realization of an unobservable <em>data-generating process</em> (DGP) from which we know nothing but the probability distribution of its constituents.</p>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<p>The simple regression model is used to study the relationship between a single independent variable and a predictor. The population model is assumed to have the following functional form:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i} \qquad (i = 1, \dots, n)
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept of the model, <span class="math inline">\(\beta_{1}\)</span> shows the average change in the dependent variable associated with a one unit change in the value of the regressor, and the error is represented by <span class="math inline">\(\epsilon_{i}\)</span>. The data does not usually stands on a perfect line, the error term <span class="math inline">\(\epsilon_{i}\)</span> represents the deviations from the linear DGP, which are assumed to follow the Normal distribution. This error term, however, is more than simple noise when it comes to estimation. Most estimation methods for the linear regression aim to minimize a function of the errors. In this article, we focus on the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares</a> (OLS) method that minimizes the <em>residual sum of squares</em> (RSS) defined as</p>
<p><span class="math display">\[
RSS = \sum_{i = 1}^{N} e_{i}^{2} = \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})^{2}
\]</span></p>
<p><label for="tufte-mn-1" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;">The Ordinary Least Squares method was developed in an era where
computational methods were not powerful enough to minimize the least
absolute deviations. The Least Squares method is also guaranteed to have
at most one solution if the number of observations is greater than the
number of independent variables.</span></span></p>
<p>The OLS estimators for the intercept and the slope are defined as the set of input values at which the minimum of <span class="math inline">\(RSS(\beta_{0}, \beta_{1})\)</span> is found.</p>
<p><span class="math display">\[
\begin{split}
\hat{\beta_{0}}, \hat{\beta_{1}} &amp;= \operatorname{argmin}_{\beta_{0}, \beta_{1}} RSS(\beta_{0}, \beta_{1})
\\
&amp;= \operatorname{argmin}_{\beta_{0}, \beta_{1}} \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})^{2}
\end{split}
\]</span></p>
<p>The solution for the first coefficient is found by setting the first derivative of the residual sum of squares with respect to <span class="math inline">\(\beta_{0}\)</span> equal to zero.</p>
<p><span class="math display">\[
\begin{split}
\frac{d RSS(\beta_{0}, \beta_{1})}{d \beta_{0}} &amp;= - 2 \sum_{i = 1}^{N} (y_{i} - \beta_{0} - \beta_{1} x_{i})
\\
0 &amp;= \sum_{i = 1}^{N} y_{i} - n \beta_{0} - \beta_{1} \sum_{i = 1}^{N}  x_{i}
\end{split}
\]</span>
The solution is then:
<span class="math display">\[
\beta_{0} = \frac{1}{n} \sum_{i = 1}^{N} y_{i} - \frac{1}{n} \beta_{1} \sum_{i = 1}^{N}  x_{i} = \bar{y} - \beta_{1} \bar{x}
\]</span>
The derivation of the estimator for <span class="math inline">\(\beta_{1}\)</span> follows the same step as the previous derivation, take the derivative and set it equal to <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\frac{d RSS(\beta_{0}, \beta_{1})}{d \beta_{1}} = \sum_{i = 1}^{N} -2 x_{i} (y_{i} - \beta_{0} - \beta_{1} x_{i})
\\
0 =  -\sum_{i = 1}^{N} x_{i} y_{i} + \beta_{0} \sum_{i = 1}^{N}  x_{i} + \beta_{1} \sum_{i = 1}^{N} x_{i}^{2}
\end{split}
\]</span></p>
<p>The solution for the intercept is substituted into the equation,</p>
<p><span class="math display">\[
\begin{split}
\beta_{1} \sum_{i = 1}^{N} x_{i}^{2} = \sum_{i = 1}^{N} x_{i} y_{i} - (\bar{y} - \beta_{1} \bar{x}) \sum_{i = 1}^{N}  x_{i}
\\
\beta_{1} \sum_{i = 1}^{N} x_{i}^{2} - \beta_{1} \bar{x} \sum_{i = 1}^{N}  x_{i} = \sum_{i = 1}^{N} x_{i} y_{i} - \bar{y} \sum_{i = 1}^{N}  x_{i}
\\
\end{split}
\]</span>
resulting in the following solution for the slope coefficient:
<span class="math display">\[
\beta_{1} = \frac{\sum_{i = 1}^{N} x_{i} y_{i} - \bar{y} \sum_{i = 1}^{N}  x_{i}}{\sum_{i = 1}^{N} x_{i}^{2} - \bar{x} \sum_{i = 1}^{N}  x_{i}}
\]</span></p>
</div>
<div id="multivariate-linear-regression" class="section level2">
<h2>Multivariate Linear Regression</h2>
<p>In the multivariate case, the model can better be represented using matrix notation in the following way:</p>
<p><span class="math display">\[
y = X \beta + \epsilon
\]</span></p>
<p>where <span class="math inline">\(y\)</span> in a vector of size <span class="math inline">\(n\)</span>, <span class="math inline">\(X\)</span> is a <span class="math inline">\(n \times m\)</span> matrix, <span class="math inline">\(\beta\)</span> is <span class="math inline">\(m\)</span>-vector, and <span class="math inline">\(\epsilon\)</span> is an other vector of size <span class="math inline">\(n\)</span>. <span class="math inline">\(m\)</span> is simply the number of independent variables <span class="math inline">\(p\)</span> plus one, thus the matrix <span class="math inline">\(X\)</span> contains <span class="math inline">\(p\)</span> regressors and a vector of ones to estimate the intercept. In matrix notation, the coefficients are also estimated by minimizing the sum of squared residuals <span class="math inline">\(e&#39;e = (y - X \beta)&#39;(y - X \beta)\)</span> with respect to <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\phi (\beta) &amp;= (y - X \beta)&#39; (y - X \beta)
\\
&amp;= y&#39;y - y&#39;X \beta - \beta&#39; X&#39;y + \beta&#39; X&#39;X \beta
\\
&amp;= y&#39;y - 2 \beta&#39; X&#39; y + \beta&#39; X&#39; X \beta
\end{split}
\]</span></p>
<p>To find the minimum, the derivative of <span class="math inline">\(\phi(\beta)\)</span> must be set equal to <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\frac{d}{d \beta}(\dots) = -2X&#39;y + 2 X&#39;X \beta &amp;= 0
\\
2X&#39;X \beta &amp;= 2X&#39;y
\\
\beta &amp;= (X&#39;X)^{-1}X&#39;y
\end{split}
\]</span></p>
<p>The <code>LinearAlgebra</code> package<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> The documentation of the package can be accessed <a href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/">here</a>.</span> makes it easy to manipulate vectors and matrices in Julia. Once the package is loaded, and the variables are defined, the next step is to create a function that takes two arguments, the response variable <span class="math inline">\(y\)</span> and the matrix <span class="math inline">\(X\)</span> of regressors with a vector of ones as the first column. The coefficients are calculated according to the least squares formula derived above.</p>
<pre class="julia"><code>function lm(y::Vector, X::Matrix)

    beta = inv(X&#39;X)X&#39;y
    return beta
end</code></pre>
<p>In the previous algorithm, the matrix <span class="math inline">\(X\)</span> is assumed to include a vector of ones as the first column, and the matrix of regressor in the next columns. The Ordinary Least Squares regression does not require the presence of an intercept to estimate the remaining coefficients.</p>
<p><span class="math display">\[
X = \begin{bmatrix}
1 &amp; x_{11} &amp; \dots &amp; x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{bmatrix}
\]</span></p>
<p>It is common practice, however, to include an intercept in the regression. The significance of its coefficient will tell us whether it is useful to keep the intercept in the model. The significance of a coefficient in a linear regression can be tested using a <strong>t-statistic</strong>. This test statistic has the following form:</p>
<p><span class="math display">\[
t_{\hat{\beta}} = \frac{\hat{\beta} - \beta}{\text{se}(\hat{\beta})} \qquad \text{where} \ H_{0}: \beta = 0
\]</span></p>
<p>The null hypothesis presented above is the standard approach in many statistical packages and simply tests for the significance of the coefficient in the model. This test can easily be extended to test for the significance of any (null) hypothesis.</p>
<pre class="julia"><code>Tstat = (beta - 0) ./ betaSE</code></pre>
<p><label for="tufte-mn-2" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;">I will not present many of the derivations because I do not know or
understand them, yet. The formula for <span class="math inline"><span class="math inline">\(\text{Var}(\hat{\beta})\)</span></span> is taken from <a
href="https://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression">Cross
Validated</a>.</span></span></p>
<p>The formula of the t-statistic includes a term that has not yet been defined, the standard error of the coefficients <span class="math inline">\(\text{se}(\hat{\beta})\)</span>. First of all, the standard error is simply the square root of the variance of the estimated coefficients. The variance of <span class="math inline">\(\hat{\beta}\)</span> depends on the variance of the error term and the inverse of the <span class="math inline">\(X&#39;X\)</span> matrix.</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}) = \hat{\sigma}^{2}_{\epsilon} (X&#39;X)^{-1}
\]</span></p>
<p>The variance of the error term is defined as</p>
<p><span class="math display">\[
\hat{\sigma}^{2}_{\epsilon} = \frac{RSS}{n - k}
\]</span></p>
<p>where the residual sum of squares is divided by the difference between the number of observation <em>(number of rows)</em> and the number of regressors + <span class="math inline">\(1\)</span> <em>(number of columns)</em> in the <span class="math inline">\(X\)</span> matrix.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">-->
<img src="lm_files/figure-html/tdistribution-1.png" alt="The **p-value** is the area of the probability density function that lies outside of the red bounds." width="672"  />
<!--
<p class="caption marginnote">-->The <strong>p-value</strong> is the area of the probability density function that lies outside of the red bounds.<!--</p>-->
<!--</div>--></span>
</p>
<pre class="julia"><code>  n = length(y)
  k = size(X, 2)
  e = y - (X * beta)

  s2 = e&#39;e / (n - k)
  betaSE = sqrt.(diag(inv(X&#39;X) * s2))</code></pre>
<p>The t-statistic is useful when it can be associated with a probability. The likelihood of the hypothesized coefficient <span class="math inline">\(\beta\)</span> can be calculated using the <strong>Student’s t-distribution</strong>. The probability of interest is <span class="math inline">\(P(x &gt; | t_{\beta}|)\)</span>, or the area that is beyond the absolute value of the t-statistic. In Julia, the algorithm to compute those probabilities can be created using the <code>Distributions</code> package.</p>
<pre class="julia"><code>using Distributions

function pt(T, df = 100) 

   pdf = TDist(df)
   Pvalue = 1 - cdf(pdf, T)
   return Pvalue
end</code></pre>
<p>Once the <code>pt()</code>function is created, the significance of each coefficient is calculated by adding this next line to the original <code>lm()</code> function. The null hypothesis <span class="math inline">\(H_{0}: \beta = 0\)</span> that no actual relationship exists between the dependent variable and the regressor of interest, and the lower the p-value, the more confidently we can reject this statement.</p>
<pre class="julia"><code>pvalue = [pt(t, n - 1) for t in tstat]</code></pre>
<p>Once all the aforementioned elements are defined, the function returns the vectors of coefficients, standard errors, t-statistics, and p-value in a dataframe. As an example, the output of a linear regression of two random variables looks as follows:</p>
<pre class="julia"><code>2×4 DataFrame
 Row │ beta     betaSE     tstat    pvalue  
     │ Float64  Float64    Float64  Float64 
─────┼──────────────────────────────────────
   1 │ 4.9808   0.0642877  77.4767      0.0
   2 │ 1.96143  0.0313061  62.6535      0.0`</code></pre>
<p>The complete algorithm can be found on <a href="https://github.com/andreghl/stats/blob/main/lm/lm.jl">GitHub</a>. The algorithm is, of course, much simpler than the actual <code>lm()</code> functions of <code>R</code>, <code>Python</code>, or <code>Julia</code>.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-Magnus" class="csl-entry">
Magnus, J. R. (2021). <em>Introduction to the theory of econometrics</em> (6th ed.). VU University Press. <a href="https://vuuniversitypress.com/product/introduction-to-the-theory-of-econometrics/?lang=en">https://vuuniversitypress.com/product/introduction-to-the-theory-of-econometrics/?lang=en</a>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
