---
title: "A Simple Regression Function"
author: "Andre-Ignace Ghonda Lukoki"
date: "March 20th, 2024"
output:
  tufte::tufte_html: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

setwd("~/Code/Portfolio/Portfolio/Andre's Portfolio/Pages/SimpleRegression")
rm(list = ls())

# To hide output, include 'results = "hide"'
```

### Introduction

For this project, I attempt to create a function for a simple regression function with one dependent and one independent variable. 

### The sample

The first step in creating a simple regression function is to create the sample on which the model will be run. This can easily be done using the ```sample()``` function. Our sample size is 30 observations and the code looks as follows:

```{r Sample creation, echo = TRUE}

set.seed(2024)

# Sample Creation

x <- rnorm(n = 300, mean = 3, sd = 2)

y <- 2*x + rnorm(n = 300, mean = 1, sd = 3)

# Plot

plot(x, y)

```

### The function

A simple regression model is an equation of the following form:

$$
\tag{1}
Y_{i} = \alpha + \beta x_{i} \quad i = 1, \dots, n
$$

This function takes two arguments. The first one is the dependent variable, usually denoted by $Y_{i}$, and the second argument is the independent variable $x_{i}$. The coefficient on the independent variable is the average effect of $x_{i}$ on $Y_{i}$ in the population. The coefficient $\alpha$ is simply the intercept of our model. To estimate both of those parameters, I will use *Ordinary Least Squares* to get the best linear predictor for $\mathbb{E}[Y|X]$.

The intercept $\alpha$ is given by the following formula:

$$
\tag{2}
\alpha = \bar{y} - \beta \bar{x}
$$

$\rightarrow$ Note that $\overline{a}$ simply denotes the mean of the random variable $a_{i}$.

The $\beta$ coefficient is given by the covariance between the two variables divided by the variable of the independent variable.

$$
\tag{3}
\beta = \frac {Cov(X,Y)}{Var(X)} 
$$

After computing both estimates, the function ```regress()``` returns them in a list. 

```{r Regression function, echo = TRUE}

regress <- function(y, x, truebeta = 0, alpha = 0.05) {
  
  dataset <- data.frame(y, x)
  
  # Sample Size
  n <- nrow(dataset)
  
  # Find mean of data
  
  meanX <- mean(dataset[,2])
  meanY <- mean(dataset[,1])
  
  # Calculating Beta 
  
  Covariance <- sum((x - meanX) * (y - meanY))
  Variance <- sum((x - meanX) ^ 2) 
  
  # Calculating Estimators 
  
  slope <- Covariance / Variance
  intercept <- meanY - slope * meanX 
  
  # Calculate error term
  
  error <- rep(0, n)
  
  for (i in 1:n){
    
    error[i] <- dataset[i, 1] - intercept -  slope * dataset[i, 2]
  }
  
  # SE of Slope
  
  var.slope <- sum(error^2) / ((n - 2) * Variance)
  
  se.slope <- sqrt(var.slope)
  
  # T(X) of Slope
  
  T.slope <- (slope - truebeta) / se.slope
  
  # P-value of T(X)
  
  p.slope <- pt(T.slope, df = n - 1, lower.tail = FALSE)
  
  # Confidence Interval for slope
  
  p <- alpha / 2
  level <- 1 - alpha
  
    # Bounds
  
  Lower <- slope - qt(p, n - 1, lower.tail = FALSE) * se.slope  
  Upper <- slope + qt(p, n - 1, lower.tail = FALSE) * se.slope 
  
 # Print Output to console
  
  cat("Regression: \n")
  cat("Method: Ordinary Least Squares \n")
  cat("___________________________________ \n")
  cat("          ", "Estimate ", "Std Error", "\n")
  cat("Intercept:", intercept, "\n")
  cat("    Slope:", slope,      se.slope, "\n")
  cat("\n")
  cat("Hypothesis Testing: \n")
  cat("----------------------------------- \n")
  cat("Null: the slope is less or equal to", truebeta, "\n")
  cat("Alt: the slope is greater than", truebeta, "\n")
  cat("\n")
  cat("          ", "T value",    "P-value", "\n")
  cat("    Slope:",  T.slope,      p.slope,  "\n")
  cat("\n")
  cat("Confidence Interval (Two-Sided): \n")
  cat("----------------------------------- \n")
  cat("The", level, "% two-sided CI for the slope is:", "\n",  "[",Lower, ";", Upper,"] \n")
  
}

```

### The results

Now it is time to test our function against the standard ```lm()``` function in R.

```{r Test, echo = TRUE}

regress(y, x)

# Test and compare output with lm()

test <- lm(y~x)
summary(test)

``` 

From the output, it is clear that both functions compute the same estimates for $\alpha$ and $\beta$ and thus we have managed to build a very simple regression function.


