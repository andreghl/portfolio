---
title: "A Simple Regression Function"
author: "Andre-Ignace Ghonda Lukoki"
date: "March 20th, 2024"
output:
  tufte::tufte_html: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# To hide output, include 'results = "hide"'
```

### Introduction

For this project, I attempt to create a function for a simple regression function with one dependent and one independent variable. 

### The sample

The first step in creating a simple regression function is to create the sample on which the model will be run. This can easily be done using the ```sample()``` function. Our sample size is 30 observations and the code looks as follows:

```{r Sample creation, echo = TRUE}

set.seed(2024)

# Sample Creation

y <- sample(1:20, 30, replace = TRUE)

x <- sample(1:20, 30, replace = TRUE)

```

### The function

A simple regression model is an equation of the following form:

$$
\tag{1}
Y_{i} = \alpha + \beta x_{i} \quad i = 1, \dots, n
$$

This function takes two arguments. The first one is the dependent variable, usually denoted by $Y_{i}$, and the second argument is the independent variable $x_{i}$. The coefficient on the independent variable is the average effect of $x_{i}$ on $Y_{i}$ in the population. The coefficient $\alpha$ is simply the intercept of our model. To estimate both of those parameters, I will use *Ordinary Least Squares* to get the best linear predictor for $\mathbb{E}[Y|X]$.

The intercept $\alpha$ is given by the following formula:

$$
\tag{2}
\alpha = \bar{y} - \beta \bar{x}
$$

$\rightarrow$ Note that $\overline{a}$ simply denotes the mean of the random variable $a_{i}$.

The $\beta$ coefficient is given by the covariance between the two variables divided by the variable of the independent variable.

$$
\tag{3}
\beta = \frac {Cov(X,Y)}{Var(X)} 
$$

After computing both estimates, the function ```regress()``` returns them in a list. 

```{r Regression function, echo = TRUE}

regress <- function(y, x) {
  
  dataset <- data.frame(y, x)
  
  # Sample Size
  n <- length(dataset)
  
  # Find mean of data
  
  meanX <- mean(dataset[,2])
  meanY <- mean(dataset[,1])
  
  # Calculating Beta 
  
  Covariance <- sum((x - meanX) * (y - meanY))
  Variance <- sum((x - meanX) ^ 2) 
  
  # Calculating Estimators 
  
  slope <- Covariance / Variance
  intercept <- meanY - slope * meanX 
  
 # Print Output to console
  
  cat("Regression: \n")
  cat("Method: Ordinary Least Squares \n")
  cat("___________________________________ \n")
  cat("          ", "Estimate",  "\n")
  cat("Intercept:", intercept, "\n")
  cat("    Slope:", slope, "\n")
  cat("----------------------------------- \n")

}

```

### The results

Now it is time to test our function against the standard ```lm()``` function in R.

```{r Test, echo = TRUE}

regress(y, x)

# Test and compare output with lm()

test <- lm(y~x)
summary(test)

``` 

From the output, it is clear that both functions compute the same estimates for $\alpha$ and $\beta$ and thus we have managed to build a very simple regression function.


